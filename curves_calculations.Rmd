---
title: "YouTube Curves Estimation"
author: "Rafal Nowicki"
date: "01 10 2020"
output:
  html_document:
    theme: united
    highlight: tango
    code_folding: hide
    toc: true
    toc_float: true
    number_sections: true
runtime: shiny
---

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri("logo.png"), 
               alt = 'logo', 
               style = 'position:absolute; top:30px; right:0; padding:30px;width: 150px;')
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)  
library(reticulate)  
knitr::knit_engines$set(python = reticulate::eng_python)  
```

# Goal

In general, our goal is to fit log-normal cumulative distribution function (CDF) into several data clouds.

Data comes from \textbf{Youtube} and represents relationship between campaign intensity (the so called `GRP`) and percent of reached audience (`REACH`). We got data on many target groups - each TG has its unique ID (`dict_id`).

There are potentially 10 types of `reach` for each TG - Reach 1+, Reach 2+, and so on, up to Reach 10+.

Interpretations of given `reach` values are as follows:

-   Reach 1+ of 0.4 means that 40% of TG was exposed to the campaign at least 1 time.
-   Reach 3+ of 0.2 means that 20% of TG was exposed to the campaign at least 3 times.

------------------------------------------------------------------------

**One important note**

Reach of higher frequency is always less than reach of lower frequency (for the same GRP).

------------------------------------------------------------------------

Theoretically, we assume investigated relationship looks something like that:

$$y_i = \beta_1 \times lognormCDF_{\mu_i, \sigma_i}(x),$$

where $y_i$ is the reach for i-th frequency and $x$ corresponds to gross rating point (GRP).

Look at the subscripts in the formula above! Scale parameter $\beta$ is estimated only once (for `REACH 1+`) and then apply for estimation of subsequent `reaches` within given TG.

Basically we do the estimation process in the following way:

For each `dict_id`:

-   Firstly we seek for 3 parameters. We use optimization algorithm to find optimal combination of parameters for which **sum of squared errors** is minimal. Estimated parameters are as follows:

| Parameter | Explanation     |
|-----------|-----------------|
| $\beta$   | scale parameter |
| $\mu$     | log mean        |
| $\sigma$  | log st. dev     |

-   Then, having the parameters' estimates for `reach 1+` we save $\beta_1$ coefficient - all of our curves within one `dict_id` will share common scale parameter. It is important because otherwise estimated curves might intersect one another. All in all for reaches of frequency higher than 1 we search for two parameters (with $\beta$ fixed):

| Parameter | Explanation |
|-----------|-------------|
| $\mu$     | log mean    |
| $\sigma$  | log st. dev |

-   We optimize `SSE` once again - this time for all reaches of higher frequency, so `reach 2+` possibly up to `reach 10+`. In that way we end up with a table looking somewhat like that:

| frequency | $\beta$ | $\mu$ | $\sigma$ | $\Sigma_i^n e_i$ |
|-----------|---------|-------|----------|------------------|
| 1         | 75      | 2.45  | 3.15     | 0.08             |
| 2         | 75      | 3.12  | 3.22     | 0.12             |
| 3         | 75      | 4.13  | 2.73     | 0.06             |
| 4         | 75      | 3.65  | 3.23     | 0.22             |
| 5         | 75      | 2.22  | 1.27     | 0.45             |
| 6         | 75      | 5.57  | 3.62     | 0.32             |
| 7         | 75      | 4.64  | 2.55     | 0.33             |

: Desired output

Then, the whole procedure is repeated for each `dict_id` in our DB.

# R functions

In this section we will compare two R functions designed to do all the tasks described above. First one uses several `for` and `while` loops - we will refer to it as the "old" one. Second one takes advantage of `tidyverse` package and especially `purrr` library that allows us to use different types of **map functions**.

Compared will be:

-   times needed to do all the calculations
-   sum of squared errors (both within `dict_id` and in total)

## Libraries

Let's take off with the analysis! First we have to load necessary packages. In fact we need just two of them - `RODBC` for DB connection and `tidyverse` for data wrangling. Remaing two libraries `DT` and `kableExtra` are used just for this document to look nice.

```{r message = FALSE}
library(RODBC)
library(tidyverse)
library(DT)
library(kableExtra)
```

## Data extraction

We configure Data Base Connection using `connect_SQL` funcion. After connecting we query DB to extract all `dict_id`. Having that done we can load all the data for `dict_id` of our interest - let's say we are going to do the analysis for the fist 1000 IDs from the DB.

```{r}
source("connect_SQL.R")

myconn <- connectSQL()

# Get the data - SQL Query ----------------------------------------------------

# Get all IDs
all.ids.df <- sqlQuery(myconn, 
                       paste0("SELECT DISTINCT [dict_id]
                                 FROM [youtube].[stage]"),
                       stringsAsFactors = F) %>%
  arrange(dict_id)

current.ids= all.ids.df[1:50, ]

n = length(current.ids)

# Get the data for given IDs
raw.current.yt.data.df <- sqlQuery(myconn, 
                                   paste0("SELECT * FROM [youtube].[stage] WHERE [dict_id] IN ('", 
                                          paste0(current.ids, collapse = "', '"),
                                          "')"), stringsAsFactors = F)
```

Now we can finally take a look on the data itself. First 150 rows look as follows:

```{r, eval = TRUE}
raw.current.yt.data.df %>%
  head(150) %>%
  datatable(., rownames = FALSE, filter="top",
            options = list(pageLength = 10, scrollX=T))
```

## Data wrangling

In order to proceed with the analysis we first have to create variables of interest - `GRP` and all the `Reaches`.

Creating `GRP` variable is quite simple - it's just division of two columns:

$$grp = 100 \times \frac{impression\_target}  {audience\_size}$$ With `reaches` it's similar but as there are several of them we will go with: `... %>% mutate(across(starts_with("reach"), ~{100 * ./audience_size})) %>% ...`

$$reach\_transformed_i = \frac{reach_i}{audience\_size} \;\textbf{ for i = 1, 2, ...}$$

```{r}
# Transform the data ----------------------------------------------------------
current.yt.data.df = raw.current.yt.data.df %>%
  mutate(grp = 100 * (impressions_target / audience_size)) %>%
  mutate(across(starts_with("reach"),
                ~{100*./audience_size})) %>%
  select(-c(audience_size, impressions, impressions_target, youtube_group_size)) 
```

## Algorithm config

Now we will set up all the necessary constraints for the optimization algorithm - on $\beta$, $\mu$ and $\sigma$.

------------------------------------------------------------------------

**One important note:** Not applying any constraints on $\beta$ might result in better fit but then we loose convenient interpretability (as the asymptotic reach (?)).

------------------------------------------------------------------------

```{r}
# Z ML estimator
#c.init = mean(log(reach1$grp))
#d.init = mean((log(reach1$grp) - c.init)^2)

# Warunki ograniczajace
c_min = -1e+2
c_max = 1e+4
d_min = 1e-8
d_max = 1e+4

# Dla frequency 2+
ui <- matrix(c(1,0, -1,0, 0,1, 0,-1),
             ncol = 2, 
             byrow = TRUE)

ci <- round(c(c_min, -c_max, d_min, -d_max), 1) 

# Dla frequency 1
ui.with.b <- cbind(ui, c(0, 0, 0, 0))
ui.with.b <- rbind(ui.with.b, matrix(c(0,0,1, 0,0,-1), ncol = 3, byrow = TRUE))
ci.with.b <- round(c(1, -500, c_min, -c_max, d_min, -d_max))
```

## R new function

This function doesn't use any loop (directly) and is based on constrOptim function. It calculates curves just once so in the output there might (or might not?) be some poor fits - we will check it...

Feel free to hit "code" button on the RHS to see the beauty of this function.

```{r}
# Reach 1 optimisation (3 parameters) -----------------------------------------
EstCurve <- function(reachCurves){
  
  
  # Funkcja celu dla frequency 1 - suma kwadratow roznic FITTED i ACTUAL
  sse <- function(params, curves.df){
    sum((params[1] * plnorm(curves.df$grp, 
                            params[2], 
                            params[3]) - curves.df$reach)^2) }
  
  
  sse_fixed_b <- function(params, curves.df){
    sum((b * plnorm(curves.df$grp, 
                    params[1], 
                    params[2]) - curves.df$reach)^2)  }
  
  
  # Initial points
  b.init = runif(1, 1, 100)
  
  # Losowe
  c.init = runif(1, 1, 100)
  d.init = runif(1, 1, 100)
  
  
  nested_df.1 = reachCurves %>%
    select(grp, reach1) %>%
    na_if(., 0) %>%
    arrange(grp) %>%
    gather("type","reach", -grp) %>%
    drop_na() %>%
    nest() %>%
    mutate(optimized = map(.x = data,
                           .f = ~constrOptim(theta = c(b.init, c.init, d.init),
                                             f = sse,
                                             curves.df = .x,
                                             grad = NULL,
                                             ui = ui.with.b,
                                             ci = ci.with.b))) %>%
    mutate(params = map(.x = optimized, .f = ~.x$par),
           sse = map_dbl(.x = optimized, .f = ~.x$value),
           maxGrp = map_dbl(.x = data, .f = ~max(.x$grp)),
           pointsNumber = map_dbl(.x = data, .f = ~nrow(.x))) %>%
    unnest_wider(params) %>%
    rename(b = ...1, c = ...2, d = ...3) %>%
    bind_cols(type = "reach1" ,.)
  
  params = nested_df.1$optimized[[1]][1]$par
  
  nested_df.1 = nested_df.1 %>%
    select(-c(optimized))
  
  # Reach 2+ optimisation ---------------------------------------
  
  b = params[1]
  
  
  nested_df = reachCurves %>%
    select(-reach1) %>%
    na_if(., 0) %>%
    arrange(grp) %>%
    gather("type","reach", -grp) %>%
    drop_na() %>%
    group_by(type) %>%
    nest() %>%
    mutate(optimized = map(.x = data,
                           .f = ~constrOptim(theta = c(c.init, d.init),
                                             f = sse_fixed_b,
                                             curves.df = .x,
                                             grad = NULL,
                                             ui = ui,
                                             ci = ci))) %>%
    mutate(b = rep(b),
           params = map(.x = optimized, .f = ~.x$par),
           sse = map_dbl(.x = optimized, .f = ~.x$value),
           maxGrp = map_dbl(.x = data, .f = ~max(.x$grp)),
           pointsNumber = map_dbl(.x = data, .f = ~nrow(.x))) %>%
    unnest_wider(params) %>%
    rename(c = ...1, d = ...2 ) %>%
    select(-c(optimized))
  
  
  # Summary table -------------------------------------------------------------
  
  out.table = rbind(nested_df.1, nested_df)
  
  return(out.table)
  
}
```

## R old function

The old function incorporates three loops and seek for the optimal solution several times (argument `maxIt`).

Again, hit the "code" button to see it.

```{r}
calculateCurvesPar2 <- function(reach.curves.df, max.iter, ui, ci, ui.with.b, ci.with.b) {
  a=1
  
  row.num <- ncol(reach.curves.df) - 1
  
  output.df <- data_frame(frequency = character(row.num),
                          a = numeric(row.num), 
                          b = numeric(row.num),
                          c = numeric(row.num),
                          d = numeric(row.num), 
                          sse = numeric(row.num),
                          maxGrp = numeric(row.num), 
                          pointsNumber = numeric(row.num))
  
  
  
  sseF2 <- function(params, curves.df, a = 1) {
    return(sum((params[3] * plnorm(a*curves.df$grp, params[1], params[2]) - curves.df$Reach)^2))
  }
  
  
  sseFixedBF <- function(params, curves.df, b, a = 1) {
    return(sum((b * plnorm(a*curves.df$grp, params[1], params[2]) - curves.df$Reach)^2))
  }
  
  
  
  for (current.reach in 1:row.num){
    
    current.reach.df <- reach.curves.df %>%
      dplyr::select('grp', Reach = paste0('reach', current.reach)) %>%
      dplyr::filter(Reach != 0)
    
    
    output.df$frequency[current.reach] <- current.reach
    output.df$pointsNumber[current.reach] <- nrow(current.reach.df)
    output.df$maxGrp[current.reach] <- round(max(current.reach.df$grp))
    
    
    if (sum(current.reach.df$Reach) == 0) {
      if (current.reach > 1) {
        output.df <- output.df[1:(current.reach - 1), ]
      } else {
        output.df <- data_frame(frequency = character(0), a = numeric(0), b = numeric(0), c = numeric(0), d = numeric(0), sse = numeric(0), maxGrp = numeric(0), pointsNumber = numeric(0))
      }
      
      return(output.df)
    }
    
    sse.temporary <- c(rep(NA, max.iter))
    acceptable.sse.found <- FALSE
    iter.count <- 0
    
    
    
    if (current.reach == 1) {
      
      par <- c(rep(NA, 3 * max.iter))
      dim(par) <- c(max.iter, 3)
      while (!acceptable.sse.found & iter.count < max.iter){
        iter.count <- iter.count + 1
        optimization <- constrOptim(theta =
                                      c(runif(1, 1, 98.9),
                                        runif(1, 1, 98.9),
                                        runif(1, 1 ,98.9)),
                                    sseF2, NULL, ui.with.b, ci.with.b,
                                    curves.df = current.reach.df)
        par[iter.count, ] <- optimization$par
        sse.temporary[iter.count] <- sseF2(par[iter.count, ], curves.df = current.reach.df, a = a)
        if ((sse.temporary[iter.count])^(0.5) / nrow(current.reach.df) < min(mean(current.reach.df$Reach) * 0.1, 1)) {
          acceptable.sse.found <- TRUE
          choice <- iter.count
        } else if (iter.count == max.iter) {
          choice <- which(sse.temporary == min(sse.temporary))[1]
        }
      }
      output.df$c[current.reach] <- par[choice, 1]
      output.df$d[current.reach] <- par[choice, 2]
      output.df$b[current.reach] <- par[choice, 3]
      output.df$a[current.reach] <- a
      
      output.df$sse[current.reach] <- sse.temporary[choice]
      
    } else {
      par <- c(rep(NA, 2 * max.iter))
      dim(par) <- c(max.iter, 2)
      b <- output.df$b[1]
      while (!acceptable.sse.found & iter.count < max.iter) 
      {
        iter.count <- iter.count + 1
        
        optimization <- constrOptim(theta = c(runif(1, 1, 98.9),
                                              runif(1, 1, 98.9)),
                                    sseFixedBF, NULL, ui, ci, curves.df = current.reach.df, b = b, a = a)
        
        par[iter.count, ] <- optimization$par
        sse.temporary[iter.count] <- sseFixedBF(par[iter.count, ], curves.df = current.reach.df, b = b, a = a)
        if ((sse.temporary[iter.count])^(0.5) / nrow(current.reach.df) < min(mean(current.reach.df$Reach) * 0.1, 1)) {
          acceptable.sse.found <- TRUE
          choice <- iter.count
        } else if (iter.count == max.iter) {
          choice <- which(sse.temporary == min(sse.temporary))[1]
        }
      }
      output.df$c[current.reach] <- par[choice, 1]
      output.df$d[current.reach] <- par[choice, 2]
      
      output.df$sse[current.reach] <- sse.temporary[choice]
      
      output.df$b[current.reach] <- b
      output.df$a[current.reach] <- a
      
    }
  }
  return(output.df)
}

```

Zapuszczamy obydwie funkcje...

```{r, message = FALSE, warning = FALSE}

# Nowa funkcja -----------------------------------------------------------------
start.time = Sys.time()

out.table = current.yt.data.df %>%
  group_by(dict_id) %>%
  group_modify(~EstCurve(.))  %>%
  mutate(type = substring(type, 6))

end.time = Sys.time()
elapsed = end.time - start.time

# Tabelki podsumowujace

bad.table = out.table %>%
  group_by(dict_id) %>%
  summarize(sse_sum = sum(sse),
            maxGrp = max(maxGrp)) %>%
  mutate(relative = sse_sum / maxGrp) %>%
  filter(relative > .5) %>%
  mutate(across(where(is.numeric), round, 2 )) %>%
  arrange(desc(relative))

sse.table = out.table %>%
  group_by(dict_id) %>%
  summarize(sse_sum = sum(sse),
            maxGrp = max(maxGrp))

n_bad = sse.table %>%
  mutate(relative = sse_sum / maxGrp) %>%
  summarise(n_bad = sum(relative > .5),
            n_not_converged = sum(relative > 10))

sse.sum = sse.table %>%
  summarise(sse.sum = sum(sse_sum))

# Stara funkcja ----------------------------------------------------------------

start.time1 = Sys.time()

out.table.old = current.yt.data.df %>%
  group_by(dict_id) %>%
  group_modify(~calculateCurvesPar2(., max.iter = 20, ui.with.b = ui.with.b, ci.with.b = ci.with.b, ui = ui, ci = ci)) 

end.time1 = Sys.time()

elapsed1 = end.time1 - start.time1 

# Tabelka podsumowujaca

bad.table.old = out.table.old %>%
  group_by(dict_id) %>%
  summarize(sse_sum = sum(sse),
            maxGrp = max(maxGrp)) %>%
  mutate(relative = sse_sum / maxGrp) %>%
  filter(relative > .5) %>%
  mutate(across(where(is.numeric), round, 2 )) %>%
  arrange(desc(relative))


sse.table.old = out.table.old %>%
  group_by(dict_id) %>%
  summarize(sse_sum = sum(sse),
            maxGrp = max(maxGrp))

n_bad.old = sse.table.old %>%
  mutate(relative = sse_sum / maxGrp) %>%
  summarise(n_bad = sum(relative > .5),
            n_not_converged = sum(relative > 10))

sse.sum.old = sse.table.old %>%
  summarise(sse.sum = sum(sse_sum))

```

## Comparison

Let's compare time needed for both functions to do all of the necessary calculations and associated Total Sums of Squared Errors.

### Summary Table

```{r}
summary_tab = elapsed %>%
  rbind(sse.sum) %>%
  cbind(elapsed1 %>%
          rbind(sse.sum.old)) %>%
  set_names(., c("new", "old")) %>%
  add_column(variable = c("time", "total SSE"), .before = T)

summary_tab %>%
  mutate_at(c(2,3), round, 2) %>%
  kable() %>%
  kable_styling() %>%
  column_spec(2, background  = ifelse(summary_tab$new > summary_tab$old ,"red", "green"), color = "white") %>%
  column_spec(3, background = ifelse(summary_tab$old > summary_tab$new ,"red", "green"), color = "white")

```

### Vizualization

```{r echo=FALSE, warning=FALSE}
inputPanel(
  sliderInput("dict_id", label = "Id",
              min = min(current.ids),
              max = max(current.ids), 
              step = 1, 
              value = min(current.ids))
  )


renderPlot({
  
  out.table %>%
    add_column(a = 1, style = "new")  %>%
    mutate(frequency = type) %>%
    bind_rows(., out.table.old %>%
                add_column(style = "old", data = out.table$data)) %>%
    filter(dict_id == input$dict_id)%>%
    unnest() %>%
    mutate(y_hat = b * plnorm(grp, c, d)) %>%
    ggplot() + 
    geom_point(aes(x = grp, y = reach, col = frequency)) +
    geom_line(aes(x = grp, y = y_hat, col = frequency)) +
    facet_grid(~style) +
    theme_bw()
})

```


***
```{r echo=FALSE, warning=FALSE}
renderText({paste0("")})


renderText({paste0("")})


output$message1 <- renderText({paste0("Old function: For ", n, " IDs there are: ", n_bad.old[1], " poor fits includinng ", n_bad.old[2], " cases where algorithm couldn't converge to optimal solution")})

output$message <- renderText({paste0("New function: For ", n, " IDs there are: ", n_bad[1], " poor fits includinng ", n_bad[2], " cases where algorithm couldn't converge to optimal solution")})

span(textOutput("message1"), style="color:grey;font-size:20px")


span(textOutput("message"), style="color:black;font-size:20px")
```



***

### Table with poor fits - both function

```{r, echo = FALSE}
renderDT({
  datatable(bad.table %>%
              full_join(bad.table.old, by = c("dict_id" = "dict_id")) %>%
              select(dict_id, sse_sum.x, sse_sum.y, relative.x, relative.y, maxGrp.x) %>%
              rename(sse_new = sse_sum.x, sse_old = sse_sum.y,
                     relative_new = relative.x, relative_old = relative.y))
})
```

```{r eval = F}

renderPlot({
  
  out.table %>%
    filter(dict_id == 4178924)%>%
    unnest() %>%
    mutate(y_hat = b * plnorm(grp, c, d)) %>%
    ggplot() + 
    geom_point(aes(x = grp, y = reach, col = type)) +
    geom_line(aes(x = grp, y = y_hat, col = type)) +
    theme_bw() +
    labs(title = paste0("Sum of squared error for id: ",
                        sse.table %>%
                          filter(dict_id == 4178924) %>%
                          select(dict_id),
                        " amounts to ",
                        sse.table %>%
                          filter(dict_id == 4178924) %>%
                          select(sse_sum) %>%
                          round(2), "\n Relative to max GRP: ",
                        sse.table %>%
                          filter(dict_id == 4178924) %>%
                          mutate(relative = sse_sum / maxGrp) %>%
                          select(relative) %>%
                          round(2)))
  
})



out.table%>%
  filter(dict_id == 4178924)%>%
  unnest() %>%
  mutate(y_hat = b * plnorm(grp, c, d)) %>%
  ggplot() + 
  geom_point(aes(x = grp, y = reach, col = type)) +
  geom_line(aes(x = grp, y = y_hat, col = type)) +
  theme_bw() +
  labs(title = paste0("Sum of squared error for id: ",
                      sse.table %>%
                        filter(dict_id == 4178924) %>%
                        select(dict_id),
                      " amounts to ",
                      sse.table %>%
                        filter(dict_id == 4178924) %>%
                        select(sse_sum) %>%
                        round(2), "\n Relative to max GRP: ",
                      sse.table %>%
                        filter(dict_id == 4178924) %>%
                        mutate(relative = sse_sum / maxGrp) %>%
                        select(relative) %>%
                        round(2)))

```

# Python

In this section we will recreate functionality of R modules described above in Python.



```{r}
toy_data_py = current.yt.data.df %>%
  filter(dict_id == 4178925) %>%
  select(-dict_id)
```


```{python, fig.align = 'center'}
from math import exp
from scipy import stats
import matplotlib.pyplot as plt

import matplotlib
matplotlib.use('TKAgg')

def lognorm_cdf(x, mu, sigma, beta):
    shape  = sigma
    loc    = 0
    scale  = exp(mu)
    return beta * stats.lognorm.cdf(x, shape, loc, scale)
  
x = r.toy_data_py.iloc[:, 10]
z = r.toy_data_py.iloc[:, 0]
y = lognorm_cdf(x, 6.03, 2.45, 100)

plt.scatter(x, y, label = "Fitted")
plt.scatter(x ,z, label = "Actual")
plt.legend(loc = 'best')

plt.show()
```



```{python, fig.align = 'center', eval = FALSE}
from math import exp
from scipy import stats
#from scipy.optimize import minimize
import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import curve_fit

import matplotlib
matplotlib.use('TKAgg')

def model(x, mu, sigma, beta):
    shape = sigma
    loc = 0
    scale = exp(mu)
    return beta * stats.lognorm.cdf(x, shape, loc, scale)

init_guess = [1, 1, 50]

fit = curve_fit(model, r.toy_data_py['grp'], r.toy_data_py['reach1'], p0 = init_guess)

ans, cov = fit
fit_mu, fit_sigma, fit_beta = ans

print(ans)

x = r.toy_data_py.iloc[:, 10]
z = r.toy_data_py.iloc[:, 0]
y = lognorm_cdf(x, fit_mu, fit_sigma, fit_beta)
plt.scatter(x, y, label = "Fitted", alpha = 0.5)
plt.scatter(x ,z, label = "Actual", alpha = 0.5)
plt.legend(loc='best')

print(sum((y - z)**2))

plt.show()
```